{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# openai-community/gpt2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading the model in locallt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from huggingface_hub import snapshot_download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name=\"openai-community/gpt2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir=\"../models\"\n",
    "folder_name=model_name.replace(\"/\",\"_\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_dir=f'{base_dir}/{folder_name}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshot_download(\n",
    "    repo_id=\"openai-community/gpt2\",\n",
    "    local_dir=local_dir,\n",
    "    local_dir_use_symlinks=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### getting the downloaded model locally and using it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_model_path='../models/openai-community_gpt2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizers=GPT2Tokenizer.from_pretrained(gpt2_model_path)\n",
    "model=GPT2LMHeadModel.from_pretrained(gpt2_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text=\"Once upon a time\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids=tokenizers.encode(\n",
    "    input_text,\n",
    "    return_tensors='pt'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output=model.generate(\n",
    "#     input_ids,\n",
    "#     max_length=50,\n",
    "#     pad_token_id=tokenizers.eos_token_id,\n",
    "#     do_sample=True,\n",
    "#     top_k=50,\n",
    "#     top_p=0.95,\n",
    "#     temperature=0.7,\n",
    "#     num_return_sequences=5,\n",
    "#     no_repeat_ngram_size=2,\n",
    "#     early_stopping=True,\n",
    "#     repetition_penalty=1.2,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    }
   ],
   "source": [
    "output=model.generate(\n",
    "    input_ids,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Once upon a time, the world was a place of great beauty and great danger. The world was a place of great'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizers.decode(output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrating with LangChain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, pipeline\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_model_path=\"../models/openai-community_gpt2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizers=GPT2Tokenizer.from_pretrained(gpt2_model_path)\n",
    "model=GPT2LMHeadModel.from_pretrained(gpt2_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizers.pad_token=tokenizers.eos_token\n",
    "model.config.pad_token_id=model.config.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "hf_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizers,\n",
    "    return_full_text=True,\n",
    "    device=0 if torch.cuda.is_available() else -1,\n",
    "    max_new_tokens=100,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    temperature=0.7,\n",
    "    repetition_penalty=1.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "hf_pipeline=pipeline(\n",
    "   \"text-generation\",\n",
    "   tokenizer=tokenizers,\n",
    "    model=model,\n",
    "    temperature=0.7\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm=HuggingFacePipeline(\n",
    "    pipeline=hf_pipeline,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=\"Write a short and imaginative story about {topic}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rajun\\AppData\\Local\\Temp\\ipykernel_24496\\3374488435.py:1: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  chain=LLMChain(\n"
     ]
    }
   ],
   "source": [
    "chain=LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = chain.run(\"A dragon who learn to code\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŒŸ Story:\n",
      " Write a short and imaginative story about A dragon who learn to code.\n",
      "\n",
      "A dragon who learn to code. Tell the story of a girl who lost her love to her dead sister.\n",
      "\n",
      "Show your interest by submitting your story to our\n",
      "\n",
      "Award Winning Short Stories Contest.\n",
      "\n",
      "Award Winning Stories Contest. Enter your story to win a prize of up to $100.\n",
      "\n",
      "This contest is open to anyone who has no interest in reading, writing or talking about short stories.\n",
      "\n",
      "Short Stories Contest\n",
      "\n",
      "(and no contest you want)\n",
      "\n",
      "A short story that's not about\n",
      "\n",
      "a person who is not a dragon.\n",
      "\n",
      "You can submit your short story at any time by clicking on the \"submit a short story\" link.\n",
      "\n",
      "If you submit your story, you will be given the title of your story, and your submission will be acknowledged and counted towards $100.\n",
      "\n",
      "If you submit your story before the deadline, you will not be entered into the contest.\n",
      "\n",
      "You will be given the title of your story by submitting it your own way.\n",
      "\n",
      "If you submit your story before the deadline, you will not be entered into the contest.\n",
      "\n",
      "If you submit your story without a title, you will not be entered into the contest.\n",
      "\n",
      "If you\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nðŸŒŸ Story:\\n\", res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
