{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain import PromptTemplate\n",
    "from langchain.callbacks import get_openai_callback\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY=os.getenv(\"OPENAI_API_KEY2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    openai_api_key=OPENAI_API_KEY,\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESPONSE_JSON = {\n",
    "    \"1\": {\n",
    "        \"mcq\": \"What is the capital of France?\",\n",
    "        \"options\": {\n",
    "            \"a\": \"Berlin\",\n",
    "            \"b\": \"Madrid\",\n",
    "            \"c\": \"Paris\",\n",
    "            \"d\": \"Rome\"\n",
    "        },\n",
    "        \"correct_option\": \"c\"\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPLATE=\"\"\" \n",
    "text={text}\n",
    "subject={subject}\n",
    "tone={tone}\n",
    "number={number}\n",
    "\n",
    "Instructios:\n",
    "You are to generate a quiz in a {tone} tone based on the subject \"{subject}\".\n",
    "the quiz should contain {number} multiple-choice question.\n",
    "Each question should have:\n",
    "- a clear and concise question text\n",
    "- four option lebeled A, B, C, D\n",
    "- a correct answer key\n",
    "\n",
    "output must be in the following JSON format:\n",
    "{response_json}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz_prompt = PromptTemplate(\n",
    "    input_variables=[\"text\", \"subject\", \"number\", \"tone\", \"response_json\"],\n",
    "    template=TEMPLATE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quiz_chain = LLMChain(\n",
    "#     llm=llm,\n",
    "#     prompt=quiz_prompt,\n",
    "#     output_key=\"quiz\",\n",
    "#     verbose=True\n",
    "# )\n",
    "\n",
    "quiz_chain= quiz_prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPLATE = \"\"\"\n",
    "You are an expert English grammarian and writer. Given a multiple-choice quiz for {subject} students,\n",
    "you need to evaluate the complexity of the questions and provide a complete analysis of the quiz.\n",
    "\n",
    "- Use **no more than 50 words** for the complexity analysis.\n",
    "- If any question does not align with the cognitive and analytical abilities of the students,\n",
    "  revise only those questions and adjust the tone to better suit the appropriate difficulty level.\n",
    "\n",
    "Quiz MCQs:\n",
    "{quiz}\n",
    "\n",
    "Please provide your evaluation and the updated quiz (if needed) from the perspective of an expert English writer:\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_prompt = PromptTemplate(\n",
    "    input_variables=[\"subject\", \"quiz\"],\n",
    "    template=TEMPLATE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# review_chain = LLMChain(\n",
    "#     llm=llm,\n",
    "#     prompt=quiz_evalution_prompt,\n",
    "#     output_key=\"review\",\n",
    "#     verbose=True\n",
    "# )\n",
    "\n",
    "review_chain= review_prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gen_evaluate_chain = SequentialChain(\n",
    "#     chains=[quiz_chain, review_chain],\n",
    "#     input_variables=[\"text\", \"subject\", \"number\", \"tone\", \"response_json\"],\n",
    "#     output_variables=[\"quiz\", 'review'],\n",
    "#     verbose=True\n",
    "# )\n",
    "\n",
    "quiz_evaluate_chain=(\n",
    "    RunnableLambda(lambda inputs:{\n",
    "        **inputs,\n",
    "        \"quiz\": quiz_chain.invoke(inputs)\n",
    "    }) |\n",
    "    RunnableLambda(lambda inputs:{\n",
    "        \"quiz\": inputs[\"quiz\"],\n",
    "        \"review\": review_chain.invoke({\n",
    "            \"quiz\": inputs[\"quiz\"],\n",
    "            \"subject\": inputs[\"subject\"]\n",
    "        })\n",
    "    })\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "file_path = r'../data.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_path, 'r') as file:\n",
    "    TEXT = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER=5\n",
    "SUBJECT=\"Data science\"\n",
    "TONE=\"Hard\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with get_openai_callback() as cb:\n",
    "#     response = gen_evaluate_chain(\n",
    "#         {\n",
    "#             \"text\":TEXT,\n",
    "#             \"number\":NUMBER,\n",
    "#             \"subject\":SUBJECT,\n",
    "#             \"tone\":TONE,\n",
    "#             \"response_json\":json.dumps(RESPONSE_JSON)\n",
    "#         }\n",
    "#     )\n",
    "\n",
    "with get_openai_callback() as cb:\n",
    "  response = quiz_evaluate_chain.invoke(\n",
    "       {\n",
    "        \"text\":TEXT,\n",
    "        \"subject\":SUBJECT,\n",
    "        \"number\":NUMBER,\n",
    "        \"tone\":TONE,\n",
    "        \"response_json\":json.dumps(RESPONSE_JSON)\n",
    "    }\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens used: 988\n",
      "Prompt Tokens: 603\n",
      "Completion Tokens: 385\n",
      "Total cose: $0.00032145\n"
     ]
    }
   ],
   "source": [
    "print(f\"Tokens used: {cb.total_tokens}\")\n",
    "print(f\"Prompt Tokens: {cb.prompt_tokens}\")\n",
    "print(f\"Completion Tokens: {cb.completion_tokens}\")\n",
    "print(f\"Total cose: ${cb.total_cost}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz_str='' # Need to conver the response[\"quiz\"] in to string "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz_dict = json.loads(quiz_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(quiz_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"data_science.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
